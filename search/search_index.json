{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/introduction/","title":"Introducing LEGENT (Alpha Version)","text":"<p>Date: 2024-05-09</p> <p>Welcome to the introduction of LEGENT, our latest venture into scalable embodied AI. LEGENT is a 3D interactive environment suitable for researchers and anyone interested to explore embodied AI in the simulated world. </p>"},{"location":"blog/introduction/#i-what-is-embodied-ai-and-whats-our-approaches","title":"I. What is Embodied AI and What's our Approaches","text":"<p>Embodied AI refers to artificial intelligence systems that interact with their environment in a way that mimics human engagement. These systems are not just virtual; they perceive, understand, and act within their surroundings, offering a richer, more integrated form of AI.</p> <p>LEGENT provides:</p> <ol> <li> <p>Content-rich simulated worlds that offer diverse embodied experiences.</p> </li> <li> <p>An intuitive interface for human-robot and human-environment interaction via keyboard and mouse, as well as language interaction through APIs of large language models or multimodal models.</p> </li> <li> <p>Scalable data generation (scene-task-trajectory) for training embodied agents.</p> </li> </ol> <p>This blog aims to guide you through the initial setup and exploration of LEGENT (without requiring you to write any code), covering only a small portion of its features. The advanced functionalities are detailed in the documentation and paper.</p>"},{"location":"blog/introduction/#ii-getting-started-with-legent","title":"II. Getting Started with LEGENT","text":"<p>To launch LEGENT, follow the steps outlined in our Installation Guide. This process will set up a default scene to get you started quickly.</p>"},{"location":"blog/introduction/#a-exploring-the-default-scene","title":"a. Exploring the Default Scene","text":"<p>Type <code>legent launch --scene 0</code> in the terminal is enough for launching the default scene. A window will pop up which displays the scene inside a two-story villa.</p> <p>You can use <code>W</code>, <code>A</code>, <code>S</code>, <code>D</code> to walk around the house, and use the mouse to adjust the perspective. You can click on the small objects around you to pick them up. If the object is out of reach, this will not take effect. You can put the object in your hand into a designated place by clicking on the place. Use <code>I</code>, <code>K</code> , <code>J</code>, <code>L</code> , <code>U</code>, <code>O</code> to adjust the pose the held object. You can also click on doors and drawers to open or close them.</p> <p>There is a robot agent in the room. In the default settings, it is not intelligent and can only perform actions based on rules. For example, you can press <code>Enter</code> to open the chatbox, and send <code>goto_user()</code> to let the robot come to you.</p> <p>You can press <code>V</code> to change the perspective of the user/robot/god. (You can try switching to the robot view and then type <code>goto_user()</code> :)</p> <p>The first-person/third-person views can be switched using <code>C</code>. The third-person view is currently only supported for the user perspective.</p>"},{"location":"blog/introduction/#b-randomizing-and-saving-scenes","title":"b. Randomizing and Saving Scenes","text":"<p>LEGENT's environment is highly customizable and allows for the large-scale random generation of scenes. </p> <p>To switch to a randomly generated scene:</p> <ol> <li> <p>Terminate the original scene and relaunch using <code>legent launch</code>.</p> </li> <li> <p>Press the <code>Enter</code> key to open the chatbox where you can type <code>#RESET</code> to initiate another new scene.</p> </li> </ol> <p>If the generated scene meets your expectations, send <code>#SAVE</code> in the chatbox. The scene will be saved as a JSON file in the <code>.legent/saved_scenes</code> folder.</p> <p>For initiating a specific scene, use <code>legent launch --scene &lt;scene-file-path or built-in-scene-id&gt;</code>. All the built-in scenes are listed here.</p>"},{"location":"blog/introduction/#iii-interaction-with-your-language-commands","title":"III. Interaction with Your Language Commands","text":"<p>Press <code>Enter</code> to open the chatbox, where you can execute various commands.</p>"},{"location":"blog/introduction/#a-built-in-functions","title":"a. Built-in Functions","text":"<p>Some built-in function are provided to control the robot.</p> <ul> <li><code>move_forward(distance)</code>. <code>move_forward(1.5)</code> will make the agent move forward 1.5 meters.</li> <li><code>rotate_right(angle)</code>. <code>rotate_right(-45)</code> will make the agent rotate to the left 45 degrees.</li> <li><code>goto_user()</code>. Let the agent come to you.</li> <li><code>goto(id)</code> where <code>id</code> the object ID. E.g., <code>goto(3)</code> will make the agent move forward 3 meters. (Press T to open the log panel, where you can see the ID of the object you are pointing)</li> <li><code>grab()</code>. Let the agent grab the object.</li> <li><code>release()</code>. Let the agent release the object.</li> </ul>"},{"location":"blog/introduction/#b-text-api-integration","title":"b. Text-API Integration","text":"<p>To enable the agent to respond to complex natural language commands, you can use your ChatGPT API key by runing <code>legent launch --scene 0 --api_key &lt;your_api_key&gt;</code>. Then, open the chatbox and send any text to the agent. See what happens.</p>"},{"location":"blog/introduction/#c-advancing-with-multimodal-api","title":"c. Advancing with Multimodal API","text":"<p>The text agent seems to perform well, but the downside is that it cannot function in unannotated environments where internal world states are inaccessible (such as other environments or the real world). Our goal is to achieve generalization across different environments and bridge the gap between virtual and real environments using embodied multimodal models. Although not powerful yet, the development is ongoing, and we expect to leverage large-scale data from LEGENT for training the models. Stay tuned!</p>"},{"location":"documentation/data/object_assets/","title":"Object Assets","text":""},{"location":"documentation/data/object_assets/#default-objects","title":"Default objects","text":"<p>To Edit</p>"},{"location":"documentation/data/object_assets/#import-your-own-3d-objects","title":"Import your own 3D objects","text":"<p>We use glTF as the standard import format. It's an openly specified 3D format that is widely supported by modern software, offers fast transmission, and can store almost all the properties of a 3D object. The environment can directly import 3D objects in <code>.gltf</code> or <code>.glb</code> format at runtime without any processing or pre-operations.</p> <p>Note</p> <p>Other formats can be easily converted to glTF using softwares like Blender. We'll create a tutorial and a one-step script to facilitate this process soon.</p>"},{"location":"documentation/data/object_assets/#minimal-example","title":"Minimal example","text":"<p>Here's a simple example of how to import an external 3D object:</p> <pre><code>from legent import Environment, ResetInfo, generate_scene\n\nenv = Environment(env_path=\"auto\")\n\n\nscene = generate_scene(room_num=1)\n# Download the 3D model from https://sketchfab.com/3d-models/lays-classic-hd-textures-free-download-d6cbb11c15ab4db4a100a4e694798279#download\n# TODO: Change this to the absolute path of the downloaded glb file, e.g., \"F:/Downloads/lays_classic__hd_textures__free_download.glb\".\npath_to_3d_model = \"path/to/lays_classic__hd_textures__free_download.glb\"\nscene[\"instances\"].append({\"prefab\": path_to_3d_model, \"position\": [1, 0.1, 1], \"rotation\": [90, 0, 0], \"scale\": [0.5, 0.5, 0.5], \"type\": \"interactable\"})\n\ntry:\n    env.reset(ResetInfo(scene))\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre> <p>Download the Lay's chips from Sketchfab or here. Simply replace <code>prefab</code> with the absolute path of your 3D model to import it into the scene.</p> <p></p>"},{"location":"documentation/data/object_assets/#advanced-example","title":"Advanced example","text":"<p>Let's try using some code to build a simple Minecraft scene. Download the grass block from Sketchfab or here. Download the workbench from Sketchfab or here .</p> <p>Create a scene using the following script:</p> Code <pre><code>from legent import Environment, Observation, ResetInfo\nimport random\n\nenv = Environment(env_path=\"auto\")\n\n# Download the 3D model from https://sketchfab.com/3d-models/minecraft-grass-block-84938a8f3f8d4a0aa64aaa9c4e4d27d3#download\n# TODO: Change this to the absolute path of the downloaded glb file, e.g., \"F:/ Downloads/minecraft_grass_block.glb\".\npath_to_grass_block = \"/path/to/minecraft_grass_block.glb\"\n\n# Download the 3D model from https://sketchfab.com/3d-models/minecraft-workbench-211cc17a34f547debb63c5a034303111#download\n# TODO: Change this to the absolute path of the downloaded glb file, e.g., \"F:/ Downloads/minecraft_workbench.glb\"\npath_to_workbench = \"/path/to/minecraft_workbench.glb\"\n\n\ndef create_simple_minecraft_scene():\n    scene = {\"instances\": []}\n\n    # Add 10*10 grass blocks\n    for x in range(-5, 5):\n        for z in range(-5, 5):\n            # Stack a random number of grass blocks\n            height = random.randint(1, 3)\n            for y in range(height):\n                scene[\"instances\"].append({\"prefab\": path_to_grass_block, \"position\": [x, y + 0.5, z], \"rotation\": [0, 0, 0], \"scale\": [0.5, 0.5, 0.5], \"type\": \"kinematic\"})\n            # Add the player on top of the grass block at (0, 2)\n            if x == 0 and z == 2:\n                scene[\"player\"] = {\"position\": [0, height, 2], \"rotation\": [0, 180, 0]}\n            # Add a workbench on top of the grass block at (0, 0)\n            elif x == 0 and z == 0:\n                scene[\"instances\"].append({\"prefab\": path_to_workbench, \"position\": [0, height + 0.5, 0], \"rotation\": [0, 0, 0], \"scale\": [1, 1, 1], \"type\": \"kinematic\"})\n            # Add the agent on top of the grass block at (0, -2)\n            elif x == 0 and z == -2:\n                scene[\"agent\"] = {\"position\": [0, height, -2], \"rotation\": [0, 0, 0]}\n    return scene\n\n\ntry:\n    obs: Observation = env.reset(ResetInfo(create_simple_minecraft_scene()))\n    while True:\n        if obs.text == \"#RESET\":\n            env.reset(ResetInfo(create_simple_minecraft_scene()))\n        obs = env.step()\nfinally:\n    env.close()\n</code></pre> <p>The code above is essentially a trivial scene generation algorithm, while Minecraft's scene generation is far more sophisticated.</p> <p></p>"},{"location":"documentation/data/object_assets/#scene-generation-integration","title":"Scene generation integration","text":"<p>To integrate your 3D objects into the default scene generation algorithm, Run:</p> <pre><code># (Optional) Add a new asset type for your asset\npython legent/scene_generation/import_external_object/add_objects.py --type asset_type --asset_type &lt;type_name&gt; --inKitchens &lt;0/1&gt; --inLivingRooms &lt;0/1&gt; --inBedrooms &lt;0/1&gt; --inBathrooms &lt;0/1&gt; --onFloor &lt;0/1&gt; --multiplePerRoom &lt;0/1&gt;\n\n# Add a new asset to a asset type\npython legent/scene_generation/import_external_object/add_objects.py --type asset --asset_type &lt;type_name&gt; --asset &lt;absolute_path_to_gltf_or_glb&gt;\n\n# For example\n# python legent/scene_generation/import_external_object/add_objects.py --type asset_type --asset_type chips --inKitchens 1 --inLivingRooms 1 --inBedrooms 1 --inBathrooms 0 --onFloor 0 --multiplePerRoom 1\n# python legent/scene_generation/import_external_object/add_objects.py --type asset --asset_type chips --asset \"F:/Downloads/lays_classic__hd_textures__free_download.glb\" \n</code></pre> <p>Run <code>legent launch</code> and send <code>#RESET</code> to generate new scenes. You should see the objects you added appear in randomly generated scenes.</p> <p>The added objects can be placed on any other object by default. The placement relationships can be found in <code>.legent/env/env_data/env_data-&lt;latest-version&gt;/procthor/receptacle.json</code>. If needed, this file can be manually edited.</p>"},{"location":"documentation/data/object_assets/#import-generated-3d-objects","title":"Import Generated 3D objects","text":"<p>With the continuous advancement of 3D generation technology, it's foreseeable that the quality of generated objects will be very high, making it an excellent supplement to existing object assets. A diverse range of objects is crucial for the generalization of embodied agents. The logic for importing generated objects is the same as for importing regular objects, because the output of 3D generation scripts also needs to conform to mainstream 3D formats.</p> <p>Here is an example for importing 3D objects generated by CRM, an advanced image-to-3D method. Download the example object from here or generate objects using the 3D generation methods yourself.</p> Code <pre><code>from legent import Environment, Observation, ResetInfo, generate_scene, get_mesh_size, convert_obj_to_gltf\nimport os\n\nenv = Environment(env_path=\"auto\")\n\n\ndef create_scene_with_generated_objects():\n    scene = generate_scene(room_num=1)\n\n    # Download the generated example from https://drive.google.com/file/d/1do5HyqUjEC76Rqg8ZSz0l8wgqHhbhUxP/view?usp=sharing\n    # Or generate the assets using the CRM model from https://github.com/thu-ml/CRM\n    # TODO: Change this to the path of the generated OBJ file, e.g., \"F:/Downloads/\u5361\u901a\u732b/tmpkiwg7ab4.obj\"\n    crm_generated_obj = \"/path/to/generated/model.obj\"\n\n    crm_converted_gltf = \"converted_example.gltf\"\n    asset_path = os.path.abspath(crm_converted_gltf)\n\n    # NOTE: Here we convert the assets in runtime. However, it is recommended to convert the assets beforehand and use the converted assets directly.\n    convert_obj_to_gltf(crm_generated_obj, crm_converted_gltf)\n    asset_size = get_mesh_size(asset_path)\n\n    scale = 0.1  # Make it smaller to resemble a toy.\n    y = asset_size[1] / 2 * scale  # Position it so that it sits right on the ground.\n\n    # Add the generated object to the scene\n    scene[\"instances\"].append({\"prefab\": asset_path, \"position\": [2, y, 2], \"rotation\": [0, 0, 0], \"scale\": [scale, scale, scale], \"type\": \"interactable\"})\n\n    return scene\n\n\ntry:\n    obs: Observation = env.reset(ResetInfo(create_scene_with_generated_objects()))\n    while True:\n        if obs.text == \"#RESET\":\n            scene = create_scene_with_generated_objects()\n            env.reset(ResetInfo(scene))\n        obs = env.step()\nfinally:\n    env.close()\n</code></pre>"},{"location":"documentation/data/object_assets/#import-academic-datasets","title":"Import Academic Datasets","text":"<p>Objaverse is a large dataset of objects that is used in many research works. The demonstration code for importing Objaverse can be found here. Note that the sizes of objects in Objaverse are inconsistent and don't match real-world settings, so resizing is necessary. This information wasn't originally included in Objaverse. We use the sizes labeled in Holodeck to resize the objects, which can be downloaded here.</p> <p></p>"},{"location":"documentation/data/scene_generation/","title":"Scene Generation","text":"<p>A generalized embodied agent needs exposure to a variety of scenes, but constructing these scenes requires extensive manual effort. LEGENT employs automatic scene generation methods and will continue to enhance them.</p>"},{"location":"documentation/data/scene_generation/#generate-a-new-scene-in-the-client","title":"Generate a new scene in the client","text":"<p>After launching the client with \"legent launch\", sending \"#RESET\" in the chat box will generate a new scene.</p>"},{"location":"documentation/data/scene_generation/#generate-a-new-scene-using-code","title":"Generate a new scene using code","text":"<p>The following code demonstrates how to end the current scene and regenerate a new one every 10 seconds.</p> <pre><code>from legent import Environment, ResetInfo, generate_scene\nfrom time import time\nenv = Environment(env_path=\"auto\")\ntry:\n    start = time()\n    env.reset()\n    while True:\n        if time() - start &gt; 10:\n            start = time()\n            env.reset(ResetInfo(generate_scene())) # Equivalent to env.reset()\n        else:\n            env.step()\nfinally:\n    env.close()\n</code></pre> <p>After calling <code>env.reset</code>, it will not return until the scene is fully loaded and rendered. <code>env.reset()</code> accepts a ResetInfo parameter, where ResetInfo.scene is the scene configuration. Below is the explanation for each field in ResetInfo.scene.</p> Key Descriptions Details instances the information of all objects For each instance, <code>instance['prefab']</code> is the prefab name of the object.  <code>instance['position']</code>, <code>instance['rotation']</code>, and <code>instance['scale']</code> represent the object's position, rotation, and scale, respectively.  <code>instance[type]</code> is the object's interaction attribute. <code>\"kinematic\"</code> means the object is unaffected by physics; <code>\"interactable\"</code> means it can be interacted with and is affected by physics. player the information of the player <code>player['position']</code>, <code>player['rotation']</code>, and <code>player['scale']</code> represent the player's position, rotation, and scale, respectively. agent the information of the agent <code>agent['position']</code> and <code>agent['rotation']</code> represent the agent's position and rotation. lights the information of all lights Optional. center The position of the panoramic top-down camera. This is the camera position for the top-down view you see when you press the V key to switch views. <p>If <code>scene</code> follows the format above, you can use <code>env.reset</code> to build the scene. There are the following sources of scenes:</p> <ol> <li>Using the default scene generation algorithm</li> <li>Using predefined scenes</li> <li>Using your own scene generation algorithm</li> <li>Manually constructed scenes or manually modified scenes based on other scenes</li> </ol> <p>The following code demonstrates manually creating a simple scene. The scene includes only a large floor, an order of potato chips, the player and the agent.</p> <pre><code>from legent import Environment, ResetInfo\nenv = Environment(env_path=\"auto\")\nscene = {\n    \"instances\": [\n        {\n            \"prefab\": \"LowPolyInterior_Floor_01\",\n            \"position\": [0, 0, 0],\n            \"rotation\": [0, 0, 0],\n            \"scale\": [4, 1, 4],\n            \"type\": \"kinematic\"\n        },\n        {\n            \"prefab\": \"LowPolyInterior_Potato\",\n            \"position\": [0,0.1,0],\n            \"rotation\": [0, 0, 0],\n            \"scale\": [1, 1, 1],\n            \"type\": \"interactable\"\n        },\n    ],\n    \"player\": {\n        \"position\": [0,0.1,1],\n        \"rotation\": [0, 180, 0]\n    },\n    \"agent\": {\n        \"position\": [0,0.1,-1],\n        \"rotation\": [0, 0, 0]\n    },\n    \"center\": [0, 10, 0],\n    \"prompt\": \"\"\n}\ntry:\n    env.reset(ResetInfo(scene))\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre> <p>The following code demonstrates using a predefined scene.</p> <pre><code>from legent import Environment, ResetInfo, load_json\nimport pkg_resources\n\nenv = Environment(env_path=\"auto\")\nscene = load_json(pkg_resources.resource_filename('legent', 'scenes/scene-default.json'))\n\ntry:\n    env.reset(ResetInfo(scene))\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre> <p>If you need to use your own scene generation algorithm, you can edit the source code of <code>legent.server.scene_generator.generate_scene</code> function.</p>"},{"location":"documentation/data/scene_generation/#debug-your-scene-generation-algorithm","title":"Debug your scene generation algorithm","text":"<p>If you write your own scene generation algorithm, it often requires repeated debugging. It would be inconvenient if you have to restart the client each time. Below is the recommended practice.</p> <p>Start the scene generation server.</p> <pre><code>legent serve\n</code></pre> <p>Start the client.</p> <pre><code>legent launch\n</code></pre> <p>Press V to change the view to the top-down view. Keep the client running. If you want to generate a new scene, send \"#RESET\" in the chat box. If your scene generation algorithm changed, just stop and restart <code>legent serve</code>.</p>"},{"location":"documentation/data/task_generation/","title":"Task Generation","text":"<p>To make embodied agents more generalizable, it's important to have a wide variety of training data, so generating different tasks automatically is very useful. For embodied agents, the two essential elements of a task are the world state and the task description. A task expressed in a certain world state gives it a grounded meaning. If it is to be used for evaluation, the task also requires a method for determining task completion. If used for training, the task requires a solution. The outputs of task generation for training are (scene, task, solution) triplets.</p> <p>To create a task, there are two approaches: first, generate a scene, and then generate a task with the scene as a condition; or first, generate a task, and then generate a scene with the task as a condition. This ensures that the task is meaningful within the context of the scene.</p> <p>Currently only several task types have been implemented :</p> <ul> <li> <p>Come here (<code>come</code>)</p> </li> <li> <p>Go to something / Stand next to something (<code>goto</code>)</p> </li> <li> <p>Pick up something (<code>take</code>)</p> </li> <li> <p>Bring me something (<code>bring</code>)</p> </li> <li> <p>Put something on something (<code>put</code>)</p> </li> <li> <p>Where is something (<code>where</code>)</p> </li> <li> <p>Is there something on something (<code>exist</code>)</p> </li> </ul> <p>This can be considered a VLA (Vision-Language-Action) version of \"T5\", integrating several tasks for joint training. The platform is developing a broader range of automatic task generation to transform \"T5\" into \"FLAN-T5\" and to bring it a step closer to an embodied \"ChatGPT\".</p>"},{"location":"documentation/data/task_generation/#generate-a-task-conditioned-on-a-scene","title":"Generate a task conditioned on a scene","text":"<p>The advantage of this approach is that a single scene can often accommodate many tasks. For high quality scenes, Large language models can generate a list of tasks conditioned on the scene, making full use of the information within it. Large language models are goot at generating brief task texts but are not good at producing long and precisely formatted scene files. Moreover, this approach does not impose any requirements on the scene generation algorithm.</p> <p>We are developing three methods for creating a task: hardcoding, prompting and chatting.</p> <ul> <li> <p>Hardcoding refers to creating specific tasks by randomly selecting objects involved and formulate the tasks by template strings. For example, when generating a <code>goto</code> task, an object such as an apple is selected, leading to the task being formulated as \"Go to the apple\".</p> </li> <li> <p>Prompting refers to creating single-turn tasks by providing a scene and task type, along with some examples of generating tasks to large language models. Large language models generate instruction following or question answering tasks conditioned on a scene. If the large language model has a sufficient understanding of physical space, the prompting method should be able to create a wide variety of tasks, not limited to several task types mentioned above.</p> </li> <li> <p>Chatting involves large language models playing both the role of the user and the role of the annotator. The large language models have complete access to the environmental state. The user, based on the environmental state, poses questions to the annotator, who then annotates the solution and executes it within the environment. This process can be iteratively performed to obtain task data in various forms. (Not implemented)</p> </li> </ul> <p>Here is a example on how to generate tasks conditioned scenes.</p> <pre><code>from legent import TaskCreator\n\ncreator = TaskCreator()\ncreator.create_tasks(task_types=['come', 'goto'], method=\"hardcoding\", scene_num=10)\n</code></pre> <p>This code will generate 10 training samples for each of the <code>come</code> and <code>goto</code> tasks, and save the samples in the <code>.legent/tasks/hardcoding</code> folder.</p> <p>For prompting, try:</p> <pre><code>from legent import TaskCreator\n\napi_key = &lt;Your ChatGPT API key&gt;\nbase_url = &lt;Base URL&gt; # None means the official url\ncreator = TaskCreator(api_key=api_key, base_url=base_url)\ncreator.create_tasks(task_types=['goto'], method=\"prompting\", scene_num=1)\n</code></pre> <p>You will see an output similar to the following including the designed prompt and ChatGPT's response. The samples will be saved in the <code>.legent/tasks/prompting</code> folder.</p> <pre><code>Send to ChatGPT:\nYou are in a room with the following objects(in table format):\nobject_id       name    position_x      position_y(vertical distance from ground)       position_z\n0       Floor   -2.50   0.00    -2.50\n1       WallFloor1      -3.75   1.25    -2.50\n2       WallFloor1      -2.50   1.25    -3.75\n3       Floor   -2.50   0.00    0.00\n4       WallFloor1      -3.75   1.25    0.00\n5       Floor   -2.50   0.00    2.50\n6       WallFloor1      -3.75   1.25    2.50\n7       WallFloor1      -2.50   1.25    3.75\n8       Floor   0.00    0.00    -2.50\n9       WallFloor1      1.25    1.25    -2.50\n10      WallFloor1      0.00    1.25    -3.75\n11      Floor   0.00    0.00    0.00\n12      WallFloor1      1.25    1.25    0.00\n13      Floor   0.00    0.00    2.50\n14      WallFloor1      1.25    1.25    2.50\n15      WallFloor1      0.00    1.25    3.75\n16      Library -1.57   1.09    1.19\n17      KitchenChair    -1.40   0.57    -2.72\n18      Library -2.96   1.09    -1.20\n19      Library 0.13    1.09    -2.02\n20      Pot     -2.20   0.15    -0.21\n21      Orange  -0.47   0.15    2.81\n22      Plate   -2.86   0.11    -0.36\n23      Plate   -0.50   0.11    0.19\n24      Potato  0.02    0.14    1.18\n25      Sandwich        -2.44   0.09    0.19\n26      Sandwich        -3.30   0.09    -0.46\n27      Cup     -2.11   0.13    0.67\n28      Bacon   -2.31   0.06    -3.53\n29      Garlic  -2.57   0.15    3.47\n30      Cupcake -2.28   0.12    0.33\n31      Cup     -3.35   0.13    0.39\nYou need to Ask the robot to go to something.\nYou need to propose 1 independent tasks and corresponding solutions, in the format of \"Task: task; Plan: plan; Solution: solution.\", with no line breaks between the three (use ';' to seperate). One sentence in Plan should match one function call in Solution.\nFor example (The examples are from other scenes. The number means object_id):\nTask: Stand next to the table.; Plan: Go to the table.; Solution: goto(32)\nTask: Can you move to the Christmas tree? I want to take a picture for you.; Plan: Go to the Christmas tree.; Solution: goto(44)\nTask: Go to the Mushroom.; Plan: Go to the Mushroom.; Solution: goto(67)\n\nReceived from ChatGPT:\nTask: Can you move to the KitchenChair?; Plan: Go to the KitchenChair.; Solution: goto(17)\n</code></pre>"},{"location":"documentation/data/task_generation/#generate-a-scene-conditioned-on-a-task","title":"Generate a scene conditioned on a task","text":"<p>This approach requires a conditional scene generation algorithm. The advantage of this approach is that it can easily generate a large number of samples for a specific task, as long as the scene generation algorithm can meet the conditions of these specific tasks.</p> <p>For instance, if you want to train the model specifically for the task \"Go to a pumpkin,\" to equip the model with this particular ability across various scenes, you can use the following code to construct the task and scenes.</p> <pre><code>from legent import generate_scene\n\ntasks = []\nfor i in range(100):\n    task = {\n        \"task\": \"Go to the Pumpkin.\",\n        \"plan\": [\"Go to the Pumpkin.\"]\n    }\n    scene = generate_scene({\"LowPolyInterior_Pumpkin\": 1}) # Ensure that the generated scene contains a pumpkin.\n    object_id = {instance['prefab']: i for i, instance in enumerate(scene['instances'])}['LowPolyInterior_Pumpkin']\n    task['solution'] = [f\"goto({object_id})\"]\n    task['scene'] = scene\n\n    tasks.append(task)\n</code></pre>"},{"location":"documentation/data/trajectory_generation/","title":"Trajectory Generation","text":"<p>A trajectory refers to a sequence of <code>observation, action, observation, action, ...</code>, which is the training data for embodied agents.</p> <p>The LEGENT environment can compute optimal controls for a solution based on the internal state of the environment. The <code>Controller</code> calculates and executes the optimal control for a solution step by step, obtaining a trajectory from the environment. Using the following code, you can directly generate training data.</p> <pre><code>from legent import Environment, ResetInfo, TaskCreator, Controller, TrajectorySaver\n\nenv = Environment(env_path=\"auto\", use_animation=False) # significantly increase the sampling rate without using animations\ntry:\n    saver = TrajectorySaver()\n    tasks = TaskCreator().create_tasks(task_types=['come', 'goto'], method=\"hardcoding\", scene_num=3) # or load from task files\n    for task in tasks:\n        env.reset(ResetInfo(scene=task['scene']))\n        controller = Controller(env, task['solution'])\n        traj = controller.collect_trajectory(task)\n\n        if traj:\n            # The task has been completed successfully\n            saver.save_traj(traj=traj)\n            print(f'Complete task \"{task[\"task\"]}\" in {traj.steps} steps.')\n        else:\n            print(f'Complete task \"{task[\"task\"]}\" failed. Deserted.')\n    saver.save()\nfinally:\n    env.close()\n</code></pre> <p>The dataset will be saved at <code>.legent/dataset</code>.</p>"},{"location":"documentation/environment/action/","title":"Action Space","text":"<p>Actions can generally be divided into planning and control. Some environments offer actions like <code>goto(object)</code>, which belong to planning. Planning actions cannot operate in unannotated scenes. The potential of control actions is significantly greater, both in terms of operational capability and generalization ability. LEGENT adopts control actions. However, LEGENT currently does not employ real robot controls, which theoretically would require precise control of each joint's rotation and more, making it overly complex for researchers not working with physical robots. While ensuring that the action is a control type, we have also simplified the control difficulty as much as possible.</p> <p>Below is the actions of an agent. Note that, along with basic move-forward action (equivalent to pressing the forward key on the keyboard), LEGENT also supports moving forward over any distance immediately (<code>teleport_forward</code>). During the continuous move-forward process, the information increment in repeatedly \"move forward, move forward, move forward\" is very small. This is not a great issue for small models. However, in cases where the computation cost is significantly high for large models during training, using the 'move forward with a distance' can greatly increase the information of the samples. When deployed for use, the model infers the distance to move forward, allowing to wait until this distance is covered before performing the next inference, effectively avoiding the huge overhead brought by inferring every frame. This action design is firstly employed by LEGENT, as a platform aimed at large models.</p> Action Descriptions Details text text send to the player string. If it is empty, it means nothing to sent. move_forward move forward in the next frame bool. If True, go forward. When <code>use_teleport==False</code>, it becomes effective. teleport_forward move forward with a distance float. The number of meters to travel forward. When <code>use_teleport==True</code>, it becomes effective. rotate_right rotate camera horizontally float. [-180, 180). Positive value means rotating right. Negative values mean rotating left rotate_down rotate camera vertically float. [-90, 90). Positive value means rotating downwards. Negative values mean rotating upwards grab grab bool. If True and the agent is holding an object, grab the object at the center of the image. If True and not holding, put the object on the surface at the center of the image api_calls api calls to the environment List[Callable]. The api returns will be put in the returned observations. <p>The types of these actions vary, but they are all expressed by codes for the model. For example: <pre><code>speak(\"OK\")\nmove_forward(2.4)\nrotate_right(35)\n</code></pre></p> <p>Below is the APIs provided to python by the environment.</p> API Descriptions Params Returns PathToUser Obtain the key points of the path to walk towards the player. The agent can walk to the player along the key points one by one in straight line without barriers in between. None api_returns['corners'] is the list of key points. PathToObject Obtain the key points of the path to walk towards an object. The index of the object in the scene config api_returns['corners'] is the list of key points."},{"location":"documentation/environment/basic_usage/","title":"Basic Usage","text":"<p>Start an environment with the following code. This code will launch the environment, establish a connection between python and the environment, and keep the environment running.</p> <pre><code>from legent import Environment\npath_to_executable = \"&lt;Your path to the environment client&gt;\" # \".legent/env/client/LEGENT-&lt;platform&gt;-&lt;version&gt;\" for example\nenv = Environment(env_path=path_to_executable) # or env_path=\"auto\" to start the latest client in .legent/env/client.\ntry:\n    env.reset()\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre> Launch the environment manuually <p>It is equal to run the following code and start the executable file in <code>.legent/env/client</code> mannually.</p> <pre><code>from legent import Environment\nenv = Environment(env_path=None)\ntry:\n    env.reset()\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre>"},{"location":"documentation/environment/basic_usage/#run-a-standard-action-observation-loop","title":"Run a standard action-observation loop","text":"<p>Once the environment is initialized, it can receive an action and return the observation after the action is completed. In this example, we initialize a random scene by calling <code>env.reset()</code> and keep the robot moving forward.</p> <pre><code>from legent import Environment, Action\n\nenv = Environment(env_path=\"auto\")\ntry:\n    obs = env.reset()\n    while True:\n        action = Action(move_forward=1) # keep moving forward\n        obs = env.step(action)\nfinally:\n    env.close()\n</code></pre> <p>All the actions can be found here.</p>"},{"location":"documentation/environment/basic_usage/#get-the-observations","title":"Get the Observations","text":"<p>In the following example, when the user sends a chat message (pressing the <code>Enter</code> key to open the chat box), we can obtain the user's input from <code>obs.text</code>, and then make a reply.</p> <pre><code>from legent import Environment, Action, Observation\n\nenv = Environment(env_path=\"auto\")\ntry:\n    obs: Observation = env.reset()\n    while True:\n        action = Action()\n        if obs.text != \"\":\n            action.text = \"I don't understand.\"\n        obs = env.step(action)\nfinally:\n    env.close()\n</code></pre> <p>When the program is running, you send a message in the chat box, and the agent is supposed to reply to you.</p> <p>You can also save what the agent sees using the following code.</p> <pre><code>from legent import save_image\nsave_image(obs.image, \"agent_view.png\")\n</code></pre> <p>All the observations can be found here.</p>"},{"location":"documentation/environment/basic_usage/#remote-communication-through-ssh","title":"Remote communication through ssh","text":"<p>Install platform toolkit on the remote server as well.</p> <p>On the remote server, run:</p> <pre><code>from legent import Environment, Action\n\nenv = Environment(env_path=None)\ntry:\n    # Do anything here.\n    # For example, we send a message to the client\n    env.reset()\n    env.step(Action(text = \"I'm on the remote server.\"))\n    while True:\n        env.step()\nfinally:\n    env.close()\n</code></pre> <p>On your personal computer, run:</p> <pre><code>legent launch --ssh &lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;\n</code></pre>"},{"location":"documentation/environment/basic_usage/#test-the-speed","title":"Test the speed","text":"<p>The environment is expected to run at 60 steps per second. This number may vary due to different machine performance. Below is the code to test the speed of the environment on your machine.</p> <pre><code>from legent import Environment\nfrom tqdm import tqdm\nfrom time import time\nenv = Environment(env_path=\"auto\")\ntry:\n    obs = env.reset()\n    steps = 1000\n    start = time()\n    for i in tqdm(range(steps)):\n        obs = env.step()\n    print(f'{steps/(time()-start):.2f} step/s')\nfinally:\n    env.close()\n</code></pre>"},{"location":"documentation/environment/humanoid_motion/","title":"Humanoid Motion","text":"<p>To Edit</p>"},{"location":"documentation/environment/observation/","title":"Observation Space","text":"<p>Below is the observations of an agent.</p> Observation Descriptions Details image the egocentric view of the agent <code>camera_resolution_width</code>*<code>camera_resolution_height</code> numpy array text the text received by the agent (i.e. what the player just send) string. If it is empty, it means nothing has been sent by the user. Note that the environment does not maintain a chat history. If needed, it should be recorded by the agent itself. <p>You are only allowed to use image and chat as input for your agents. This is necessary to ensure the generalizability of the agent. However, during training or data generation you are allowed to use additional info from the environment. This information is returned along with the observation, with the content as follows.</p> Observation Descriptions Details game_states all the game inner states json object(Dict). api_returns the returns of the api_calls in the last action json object(Dict). <p>Below is the explanation for each field in game_states.</p> Key Descriptions Details instances the information of all objects <code>obs['instances'][i]['prefab'</code>] is the prefab name of the object. <code>obs['instances'][i]['position']</code> is the position of the object. <code>obs['instances'][i]['forward']</code> the direction the object is facing. player the information of the player <code>obs['player']['position']</code> is the position of the player. <code>obs['player']['forward']</code> the direction the player is facing. agent the information of the agent <code>obs['agent']['position']</code> is the position of the agent. <code>obs['agent']['forward']</code> the direction the agent is facing player_camera The position of the player's camera, from which the egocentric image is obtained. <code>obs['player_camera']['position']</code> is the position of the camera. <code>obs['player_camera']['forward']</code> the direction the camera is facing. agent_camera The position of the agent's camera, from which the egocentric image is obtained. <code>obs['agent_camera']['position']</code> is the position of the camera. <code>obs['agent_camera']['forward']</code> the direction the camera is facing. player_grab_instance The index of the object that the player has grabbed. <code>\"player_grab_instance\": i</code> means instances[i] is grabbed. agent_grab_instance The index of the object that the agent has grabbed. <code>\"agent_grab_instance\": i</code> means instances[i] is grabbed. <p>This information is useful, for instance, for spatial calculations, determining task completion, or calculating rewards.</p>"},{"location":"documentation/environment/rendering_settings/","title":"Rendering Settings","text":"<p>To Edit</p>"},{"location":"documentation/getting_started/demo_video/","title":"Demo Video","text":"<p>Here is a demo video of the environment.</p>"},{"location":"documentation/getting_started/installation/","title":"Installation","text":"<p>LEGENT supports Windows/Linux/MacOS/Web.</p>"},{"location":"documentation/getting_started/installation/#install-platform-toolkit","title":"Install Platform Toolkit","text":"<p>It is recommended to use Conda to manage the environment:</p> <pre><code>conda create -n legent python=3.10\nconda activate legent\n</code></pre> <p>Install the toolkit: <pre><code>git clone https://github.com/thunlp/LEGENT\ncd LEGENT\npip install -e .\n</code></pre></p> <p>If your network cannot access GitHub, use ssh to clone the repository:</p> <pre><code>git clone git@ssh.github.com:thunlp/LEGENT.git\n</code></pre>"},{"location":"documentation/getting_started/installation/#download-environment-client","title":"Download Environment Client","text":""},{"location":"documentation/getting_started/installation/#download-automaticlly-recommended","title":"download automaticlly (recommended)","text":"<p>After installing the toolkit, run the following command:</p> <pre><code>legent download\n</code></pre> <p>If your network cannot access Huggingface Hub, run:</p> <pre><code>legent download --thu\n</code></pre> <p>It will automatically download the latest version of the client for your system to the <code>.legent/env</code> folder in the current directory.</p>"},{"location":"documentation/getting_started/installation/#download-manually-not-recommended","title":"download manually (not recommended)","text":"<p>If auto-download fails, you can manually download the client. Download from Huggingface Hub or Tsinghua Cloud. After downloading, extract the file to create the following file structure:</p> <pre><code>LEGENT/\n\u2514\u2500\u2500 .legent/\n    \u2514\u2500\u2500 env/\n        \u251c\u2500\u2500 client\n        \u2502   \u2514\u2500\u2500 LEGENT-&lt;platform&gt;-&lt;version&gt;\n        \u2514\u2500\u2500 env_data/\n            \u2514\u2500\u2500 env_data-&lt;version&gt;\n</code></pre> <p>For Linux or MacOS users, you need to modify file permissions:</p> <pre><code>chmod -R 777 .legent/env\n</code></pre>"},{"location":"documentation/getting_started/installation/#run-on-linux-server-optional","title":"Run On Linux Server (Optional)","text":"<p>Normally, we train and deploy models on Linux servers, which often do not have a display. We recommend the following methods: </p> <ol> <li> <p>Use the remote communication function of LEGENT. Use the environment on your local computer and train the model on the server.</p> <p>We recommend using this method first, as it is very convenient for visually using and debugging.</p> </li> <li> <p>Use Xvfb.</p> <p>If you do not wish to be bottlenecked by network performance, such as when needing large-scale parallel sampling, you need to use Xvfb on the Linux servers to support rendering.</p> <p>If you have root access to the server, you can install Xvfb by using: <pre><code>sudo apt install xvfb\n</code></pre></p> <p>Prepend <code>xvfb-run</code> to commands that run Python scripts. For example: <pre><code>xvfb-run python demo.py\n</code></pre></p> </li> <li> <p>Use Docker.</p> <p>Not ready.</p> </li> </ol>"},{"location":"documentation/getting_started/introduction/","title":"Introduction","text":"<p>In the future, robots will perceive the environment as we do, communicate with us through natural language and help us with our tasks. LEGENT is dedicated to developing robots that can chat, see, and act from virtual worlds to the real world. Designed to integrate large models with embodied agents, this platform prioritizes ease of use and scalability, focusing on developing:</p> <ol> <li> <p>An easy-to-use environment that simulates a physical world, where an agent can interact with humans through language, receive egocentric vision, and perform physical actions.</p> </li> <li> <p>Automated generation of training data, including the generation of scenes, tasks, and agent trajectories. The platform is tailored to train large multimodal models as embodied models, using generated data from simulated worlds at scale. LEGENT serves as the data engine for embodied models in robotics and games, as well as for world models.</p> </li> </ol> <p>Important Note</p> <p>LEGENT is currently organizing code and documents, as well as carrying out necessary bug fixes and improvements to existing features. It will be more convenient to use once this process is complete. If you want a more stable version, please stay tuned!</p> <p>Note</p> <p>LEGENT is in the early stages of development, so issues and shortcomings are inevitable. We appreciate your constructive feedback and will address problems as quickly as possible.</p>"},{"location":"documentation/getting_started/play/","title":"Play","text":""},{"location":"documentation/getting_started/play/#start-the-environment-client","title":"Start the Environment Client","text":""},{"location":"documentation/getting_started/play/#in-one-command-recommended","title":"in one command (recommended)","text":"<pre><code>legent launch --scene 0\n</code></pre> <p>This command will launch the latest client in the .legent/env/client folder. If you want to launch a specific client, run:</p> <pre><code>legent launch --env_path &lt;path-to-the-client&gt;  --scene 0\n</code></pre> <p>A predefined 3D scene will be displayed, where you and a robot agent are positioned in a first-person view.</p> A reminder for Windows users <p>If you're using a Windows computer with a VPN enabled, please disable it. Otherwise, the client cannot get the scene file from the launched scene server. We'll investigate this issue further in future updates.</p>"},{"location":"documentation/getting_started/play/#step-by-step-manually","title":"step by step manually","text":"<p>The scenes in LEGENT are all generated on the Python side. The client side obtains scenes by requesting the scene server (or using <code>legent.Environment</code> in the toolkit). Start the scene generator by running:</p> <pre><code>legent serve\n</code></pre> <p>To start the client, simply open it like any regular software. For example, by double-clicking the executable file.</p> <p>Or enter the following command in the console: <pre><code>\"&lt;path-to-the-executable-file&gt;\" --width &lt;screen-width&gt; --height &lt;screen-height&gt;\n</code></pre></p>"},{"location":"documentation/getting_started/play/#manual-controll","title":"Manual Controll","text":""},{"location":"documentation/getting_started/play/#character-controll","title":"character controll","text":"<p>Move the mouse to rotate the perspective. Press W, A, S, D to move the character. Press G or left mouse button to grab/release an object. Press Enter to chat.</p>"},{"location":"documentation/getting_started/play/#view-controll","title":"view controll","text":"<p>Press C to switch between the first-person-view and the third-person-view. Press V to switch between your view, the agent's view, and a panoramic top-down view. Press X to switch between full screen and windowed screen. Press Esc to unfocus the game client.</p> <p>Currently the robot will not have any response. Next, we will use the Python side to control the robot.</p>"},{"location":"documentation/getting_started/play/#default-scenes","title":"Default Scenes","text":"<p>Currently, LEGENT has two default scenes: 0 (stylized) and 1 (realistic). You can use these scenes by running <code>legent launch --scene 0</code> or legent <code>launch --scene 1</code> respectively.</p>"},{"location":"documentation/getting_started/web_demo/","title":"Instructions for LEGENT web demo (Alpha Version)","text":""},{"location":"documentation/getting_started/web_demo/#how-to-play","title":"How to play","text":"<p>Move the mouse: Rotate the perspective.</p> <p>W, A, S, D: Move the character.</p> <p>G or left mouse button: Grab, release, open, or close an object.</p> <p>I, K or J, L or U, O: Rotate a held object in three dimensions. (experimental)</p> <p>Enter: Open the chatbox to chat.</p>"},{"location":"documentation/getting_started/web_demo/#chat","title":"Chat","text":"<p>Type anything into the chat box and press enter to send. The agent's control sequence will be displayed in the upper left corner (press T to hide)</p>"},{"location":"documentation/getting_started/web_demo/#simple-control","title":"Simple Control","text":"<p>Send \"move_forward(1.2)\" to make the agent move forward 1.2m. Send \"rotate_right(30)\" to make the agent rotate right 30 degrees.</p>"},{"location":"documentation/getting_started/web_demo/#generate-a-new-scene","title":"Generate a new scene","text":"<p>Send \"#RESET\" in the chatbox to generate a new scene.</p>"},{"location":"documentation/getting_started/web_demo/#important-notes","title":"Important notes","text":"<ol> <li>The web demo has much poorer physics and lower frame rates compared to the desktop versions, which may cause some flawed physical behaviors.</li> <li>The current scene generation is from an old version with very limited objects. A better version will be released soon.</li> <li>The chat ability is still very primitive and the agent will make errors. We are developing it.</li> <li>The web demo only showcases a small portion of the platform's functionality. It uses stylized rendering and low-poly object assets instead of realistic ones. For more features, please refer to the platform's documentation.</li> </ol>"},{"location":"documentation/model/evaluation/","title":"Evaluation","text":"<p>Unlike traditional tasks that compare the model's output with the ground truth on datasets, embodied intelligence requires interaction with the environment and uses the world state to determine if a task has been completed. The platform provides manual interaction for qualitative evaluation, as well as automatic quantitative evaluation.</p>"},{"location":"documentation/model/evaluation/#interact-with-your-model","title":"Interact with your model","text":"<p>By interacting with the model directly in the environment's chat box, we can intuitively experience the model's performance. After training the model, deploy it by running:</p> <pre><code>MODEL_PATH=&lt;model_path&gt; python scripts/llava/serve.py\n</code></pre> <p>On your personal computer, launch the client by running:</p> <pre><code>python scripts/interact_with_model.py --ssh &lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;,&lt;password&gt;\n</code></pre> <p>Here <code>&lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;,&lt;password&gt;</code> is how you login the remote server through SSH. <code>:&lt;ssh_port&gt;</code> and <code>,&lt;password&gt;</code> are optional.</p> <p>Chat with the model in the chatbox and see its actions.</p>"},{"location":"documentation/model/evaluation/#interact-with-gpt4-v","title":"Interact with GPT4-V","text":"<p>On your personal computer, launch the client by running:</p> <pre><code>python scripts/interact_with_model.py --api_key &lt;api_key&gt; --base_url &lt;base_url&gt;\n</code></pre> <p>Here <code>--api_key &lt;api_key&gt; --base_url &lt;base_url&gt;</code> is how you use the GPT4-V API. <code>--base_url &lt;base_url&gt;</code> is optional.</p> <p>Chat with GPT4-V in the chatbox and see its actions.</p>"},{"location":"documentation/model/evaluation/#evaluate-your-model","title":"Evaluate your model","text":"<p>To fairly compare models and ensure the reproducibility of experiments, we first generate some task settings and then consistently use these settings for subsequent evaluations.</p> <p>Generate task settings. Use <code>--task come</code> for \"Come here\" task and <code>--task where</code> for \"Where is the orange\" task. </p> <pre><code>python scripts/create_eval.py --task come --num 10\n</code></pre> <p>The generated task settings will be saved at  <code>.legent/eval</code>.</p> <p>After training and deploying the model, run the following script on your personal computer:</p> <pre><code>python scripts/eval_model.py --task come --ssh &lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;,&lt;password&gt;\n</code></pre> <p>All the informations and results will be saved at <code>'.legent/eval/.../results/...-model'</code></p>"},{"location":"documentation/model/evaluation/#evaluate-gpt4-v","title":"Evaluate GPT4-V","text":"<p>run the following script on your personal computer:</p> <pre><code>python scripts/eval_model.py --task come --api_key &lt;api_key&gt; --base_url &lt;base_url&gt;\n</code></pre> <p>All the informations and results will be saved at <code>'.legent/eval/.../results/...-gpt4v'</code></p>"},{"location":"documentation/model/multi_image/","title":"Multi-Image VLA","text":"<p>We use VILA for training a VLA model with multi-image input.</p> <p>Note</p> <p>The models for multi-image or video input are still in its early stages, the platform is urgently experimenting. Currently, VILA can only handle a maximum of six images. We use a window of six images to input the observations.</p>"},{"location":"documentation/model/multi_image/#training","title":"Training","text":"<p>Clone repository:</p> <pre><code>git clone https://github.com/Efficient-Large-Model/VILA\ncd VILA\n</code></pre> <p>Install VILA:</p> Install VILA <pre><code>conda create -n vila python=3.10 -y\nconda activate vila\n\npip install --upgrade pip  # enable PEP 660 support\nwget https://github.com/Dao-AILab/flash-attention/releases/download/v2.4.2/flash_attn-2.4.2+cu118torch2.    0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\npip install flash_attn-2.4.2+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\npip install -e .\npip install -e \".[train]\"\n\npip install git+https://github.com/huggingface/transformers@v4.36.2\ncp -rv ./llava/train/transformers_replace/* ~/anaconda3/envs/vila/lib/python3.10/site-packages/transformers/    models/\n</code></pre> <p>Generate the training data we need. </p> Data Generation <p>On the remote server, run:</p> <pre><code>xvfb-run python scripts/create_traj_come.py\n</code></pre> <p>or </p> <pre><code>xvfb-run python scripts/create_traj_where.py\n</code></pre> <p>The dataset will be saved at <code>.legent/dataset</code> on the remote server.</p> <p>To Edit</p>"},{"location":"documentation/model/single_image/","title":"Single-Image VLA","text":"<p>Here, the training and deployment example can only handle single-image input, taking LLaVA as an example base model. To quickly illustrate the process, the training and deployment example only uses the task of \"come here\". However, to enable the model to have a general capability, it should be trained on a variety of tasks.</p> <p>Note</p> <p>Embodied models should have the capability to input multiple sequential images or videos to get a good understanding of the environment. If we use a single-image model, it means that the agent cannot perceive and memorize the observation history well. Without the observation history, the agent will encounter many problems, such as getting easily stuck. The problems are not very significant for tasks where the camera does not move a lot, such as robotic arm operations, but it has a major impact on tasks that involve navigation.</p>"},{"location":"documentation/model/single_image/#training","title":"Training","text":"<p>Install dependencies on the remote server:</p> <pre><code>pip install -e \".[llava]\"\npip install flash-attn --no-build-isolation\n</code></pre> <p>Generate the training data we need.</p> Data Generation <p>You can put the training data on the server using one of the following three methods:</p> <ol> <li> <p>Generate trajectory on your personal computer and upload to the remote server manually.</p> </li> <li> <p>Generate trajectory on the remote server using Xvfb.</p> <p>On the remote server, run:</p> <pre><code>xvfb-run python scripts/create_traj_come.py\n</code></pre> </li> <li> <p>Using ssh.</p> <p>On the remote server, run:</p> <pre><code>python scripts/create_traj_come.py\n</code></pre> <p>On your personal computer, run:</p> <pre><code>legent launch --ssh &lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;,&lt;password&gt;\n</code></pre> <p>The dataset will be saved at <code>.legent/dataset</code> on the remote server.</p> </li> </ol> <p>Prepare the training data to LLaVA format.</p> <pre><code>python scripts/llava/prepare_dataset.py\n</code></pre> <p>Download the model.</p> <pre><code>python scripts/llava/download_model.py\n</code></pre> <p>The model will be downloaded to <code>.legent/models/base/llava-v1.5-7b</code>.</p> <p>Train the model by running:</p> <pre><code>bash scripts/llava/train.sh\n</code></pre> <p>The save path will be printed.</p>"},{"location":"documentation/model/single_image/#deployment","title":"Deployment","text":"<p>On the remote server, deploy the model by running:</p> <pre><code>MODEL_PATH=&lt;model_path&gt; python scripts/llava/serve.py\n</code></pre> <p>On your personal computer, launch the client by running:</p> <pre><code>from legent import Environment, AgentClient\n\nenv = Environment(env_path=\"auto\")\nagent = AgentClient(ssh=\"&lt;username&gt;@&lt;host&gt;:&lt;ssh_port&gt;\")\nobs = env.reset()\ntry:\n    while True:\n        action = agent.act(obs)\n        obs = env.step(action)\nfinally:\n    env.close()\n    agent.close()\n</code></pre> <p>In the chatbox, input \"Come here\" and see what happens.</p>"},{"location":"documentation/model/vla/","title":"Vision-Language-Action","text":"<p>Large language models (LLM) are trained on massive amounts of text. LLMs contain certain world knowledge and exhibit sparks of general intelligence. However, they lack capabilities in embodied perception and control. Vision language models (VLM) typically start with LLMs and incorporate image-text data for training, enabling them to perceive images.</p> <p>A practical step towards obtaining a rudimentary general-purpose embodied model is to start with a VLM and train it with embodied data, such as robot trajectories. This process equips the model with the capability for embodied perception and control within physical environments. The further trained VLMs are known as vision-language-action (VLA) models. VLAs have the potential to obtain a range of emergent capabilities. Read the paper RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control to learn more.</p> <p>The availability of open-source VLA models is currently limited. LEGENT has created embryonic VLA models leveraging open-source foundational models and generated data. Large-scale training with more tasks and trajectories is in progress.</p> <p>We demonstrated how to train single-image or multi-image models with generated trajectories.</p>"}]}